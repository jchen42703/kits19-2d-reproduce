from os.path import join, isdir
from tqdm import tqdm
from pathlib import Path
import os
import numpy as np
import inspect
import torch

from kits19cnn.inference.utils import remove_3D_connected_components
from kits19cnn.inference.predictors import BasePredictor
from kits19cnn.io import get_bbox_from_mask, expand_bbox, crop_to_bbox, resize_bbox
from kits19cnn.utils import load_json, save_json

class Stage1Predictor(BasePredictor):
    """
    Inference for a single model for every file generated by `test_loader`.
    Predictions are saved in `out_dir`.

    Predictions are done on the resized predictions.
    Post-processing
      * Foreground classes merged after prediction.
      * The ROIs to segment for Stage 2 were extracted creating a bounding box
        to circumscribe the union of the kidney plus the tumor (or just the
        kidneys) by using the (x, y, z) coordinates from the first stage
        segmentations.
      * After that, the bounding box was symmetrically expanded to reach the
        final size of 256Ã—256 pixels.
        * Avoids interpolation process on the extracted images.

    """
    def __init__(self, out_dir, model, test_loader, scale_ratios_json_path,
                 pseudo_3D=True, pred_3D_params={"do_mirroring": True}):
        """
        Attributes
            out_dir (str): path to the output directory to store predictions
            model (torch.nn.Module): class with the `predict_3D` method for
                predicting a single patient volume.
            test_loader: Iterable instance for generating data
                (pref. torch DataLoader)
                must have the __len__ arg.
            scale_ratios_json_path (str): Path to the .json generated by
                `scripts/utility/create_scale_ratio_dict.py`
            pred_3D_params (dict): kwargs for `model.predict_3D`
            pseudo_3D (bool): whether or not to have pseudo 3D inputs
        """
        super().__init__(out_dir=out_dir, model=model, test_loader=test_loader)
        assert inspect.ismethod(model.predict_3D), \
                "model must have the method `predict_3D`"
        if pseudo_3D:
            assert inspect.ismethod(model.predict_3D_pseudo3D_2Dconv), \
                "model must have the method `predict_3D_pseudo3D_2Dconv`"
        self.pred_3D_params = pred_3D_params
        self.pseudo_3D = pseudo_3D
        self.bbox_coords = {}
        self.scale_ratios_dict = load_json(scale_ratios_json_path)

    def run_3D_predictions(self, min_size=5000):
        """
        Runs and saves predictions on the dataset (specified in test_loader),
        post-processes, creates and save the bounding boxes.
        Args:
            min_size (int): The minimum size for a 3D connected components to
                be considered as part of the ROIs.
        Returns:
            None
        """
        cases = self.test_loader.dataset.im_ids
        assert len(cases) == len(self.test_loader)
        for (test_batch, case) in tqdm(zip(self.test_loader, cases), total=len(cases)):
            test_x = torch.squeeze(test_batch[0], dim=0)
            if self.pseudo_3D:
                pred, _, act, _ = self.model.predict_3D_pseudo3D_2Dconv(test_x,
                                                                    **self.pred_3D_params)
            else:
                pred, _, act, _ = self.model.predict_3D(test_x,
                                                        **self.pred_3D_params)
            assert len(pred.shape) == 3
            assert len(act.shape) == 4
            pred = remove_3D_connected_components(pred, min_size=min_size)
            pred = self.post_process_stage1(pred)
            self.save_pred(pred, act, case)
            case_raw = Path(case).name
            bbox_coord = self.create_bbox_stage1(pred, case_raw)
            self.bbox_coords[case_raw] = bbox_coord
        self.save_bbox_coords()

    def post_process_stage1(self, pred):
        """
        Foreground classes merged after prediction.
        """
        return (pred>0)*1

    def create_bbox_stage1(self, pred, case_raw):
        """
        Creates the bounding box from the prediction. Resizes the bbox based
        on the scale ratio (original / resized) and then expands the spatial
        dimensions (x & y) of the bbox to 256 x 256.
        Args:
            pred (np.ndarray): 3D Array (no channels)
            case_raw (str): Raw case name
        Returns:
            expanded_bbox (list):
                [[lb_x, ub_x], [lb_y, ub_y], [lb_z, ub_z]]
                where lb -> lower bound coordinate
                      ub -> upper bound coordinate
        """
        bbox = get_bbox_from_mask(pred, outside_value=0)
        resized_bbox = resize_bbox(bbox,
                                   self.scale_ratios_dict[case_raw])
        expanded_bbox = expand_bbox(resized_bbox,
                                    bbox_lengths=[None, 256, 256])
        return expanded_bbox

    def save_bbox_coords(self):
        """
        Saves the bbox coords dictionary as a .json.
        """
        out_path = join(self.out_dir, "bbox_stage1.json")
        save_json(self.bbox_coords, out_path)
        print(f"Saved the bounding box coordinates at {out_path}.")
